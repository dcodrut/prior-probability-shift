{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Online Local Adaptive Model\n",
    "\n",
    "* Prior Probability Shift is one of the common problems encountered in Machine Learning algortihms.   \n",
    "* There are some approaches for dealing with this problem in a 'static' scenario. But there are situations in which we need a model which deals with secvential data as input (e.g. a server which gets input from different users, with different data distributions).   \n",
    "* In this project, we try to build a model which self adapts its predictions based on the local label distribution. \n",
    "\n",
    "### About notebook 2\n",
    "\n",
    "In this notebook we address the problem of Prior Probability Shift as following:\n",
    "- we ilustrate an example of how a different test distribution has an impact on the model's performance.\n",
    "We train multiple models, on a range of subsets of MNIST, with different distributions. Then each model is tested on a range of test subsets with respect to the distributions considered in the training phase.\n",
    "- then, we test that imposing the real priors helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Notebook setup and data preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "# %matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "import pickle\n",
    "from training_plotter import TrainingPlotter\n",
    "from dataset import MNISTDataset\n",
    "import utils\n",
    "from lenet5 import Lenet5\n",
    "from lenet5_with_distr import Lenet5WithDistr\n",
    "import PIL.Image\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(edgeitems=10)\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random generator using a constant seed in order to reproduce results\n",
    "seed = 112358\n",
    "nprg = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_TRAIN_IMAGES_FILEPATH = 'MNIST_dataset/train-images.idx3-ubyte'\n",
    "MNIST_TRAIN_LABELS_FILEPATH = 'MNIST_dataset/train-labels.idx1-ubyte'\n",
    "MNIST_TEST_IMAGES_FILEPATH = 'MNIST_dataset/t10k-images.idx3-ubyte'\n",
    "MNIST_TEST_LABELS_FILEPATH = 'MNIST_dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train models on subsets with different distributions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build some label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distrs_used_for_training = []\n",
    "\n",
    "# uniform distribution\n",
    "distr = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "# normal distribution centered about label 4-5\n",
    "r = 2\n",
    "distr = [r**1,r**2,r**3,r**4,r**5,r**5,r**4,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# skewed normal distribution centered about 2\n",
    "distr = [r**3,r**4,r**5,r**4.5,r**4,r**3.5,r**3,r**2.5,r**2,r**1.5]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# skwed normal distribution centered about 7\n",
    "distr = [r**1.5,r**2,r**2.5,r**3,r**3.5,r**4,r**4.5,r**5,r**4,r**3]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# bimodal normal distribution\n",
    "distr = [r**1,r**2,r**3,r**2,r**1,r**1,r**2,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# bimodal skewed normal distribution\n",
    "distr = [r**3.5,r**4,r**3,r**2,r**1,r**1,r**2,r**3,r**4,r**3.5]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "\n",
    "# exponential distribution\n",
    "r=1.4\n",
    "distr = [r**1,r**2,r**3,r**4,r**5,r**6,r**7,r**8,r**9,r**10]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# exponential distribution\n",
    "r=1.4\n",
    "distr = [r**10,r**9,r**8,r**7,r**6,r**5,r**4,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "print('#distributions used for training = {}'.format(len(distrs_used_for_training)))\n",
    "for idx, distr in enumerate(distrs_used_for_training):\n",
    "    print('idx = {}: distr = {}'.format(idx,distr))\n",
    "    plt.bar(range(10), distr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LeNet5 models by imposing the considered distributions on the original MNIST dataset, with fixed subset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE_LIST = [150, 250, 500, 1000, 5000, 10000]\n",
    "global_max_weight = np.max(distrs_used_for_training)\n",
    "\n",
    "for subset_size in SUBSET_SIZE_LIST:\n",
    "    for k, distr in enumerate(distrs_used_for_training):\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "        print('\\n\\nk = {}: Imposed distribution: {}'.format(k, np.round(np.array(distr), decimals=3)))\n",
    "        mnist_ds.impose_distribution(np.array(distr), global_max_weight, max_training_size=subset_size)\n",
    "        lenet5_model = Lenet5(mnist_ds, \"with_imposed_distr_{}_{}samples\".format(k, subset_size), epochs=40, batch_size=100, variable_mean=0, variable_stddev=0.1, learning_rate=0.001, drop_out_keep_prob=0.75)\n",
    "        lenet5_model.train()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Test the above models and adjust outputs using real priors\n",
    "Test each model:\n",
    "- on a subset which respects its training distribution\n",
    "- on subsets which respects the other models' distributions\n",
    "- on the entire MNIST test set\n",
    "\n",
    "In order to summarize the results, we will build a matrix, similar to a confusion matrix, by following the next steps:\n",
    "- take each trained model and test it again on his initial test set\n",
    "- then test each model on the data that respects, by turn, the other distributions considered\n",
    "- save these results into a dictionary and then to disk for reuse them later\n",
    "- finally, build a matrix where an element $m_{ij}$ represents the test accuracy of model $i$ evaluated on the subset with distribution $j$  \n",
    "\n",
    "Similar, some matrices containing label distribution are built (for correct predictions, wrong predictions, wrong actual predictions)\n",
    "\n",
    "Formula for corrected a posteriori probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    " \\large\n",
    " \\hat{p}(\\omega_{i}|\\mathrm{x})=\\frac{\\frac{\\hat{p}(\\omega_{i})}{\\hat{p}_{t}(\\omega_{i})}\\hat{p}_{t}(\\omega_{i}|\\mathrm{x})}{\\sum_{j=1}^{n}\\frac{\\hat{p}(\\omega_{j})}{\\hat{p}_{t}(\\omega_{j})}\\hat{p}_{t}(\\omega_{j}|\\mathrm{x})}\n",
    "\\end{equation}\n",
    "\n",
    "- $\\hat{p}(\\omega_{i}|\\mathrm{x})$ = corrected a posteriori probabilities\n",
    "- $\\hat{p}_{t}(\\omega_{i}|\\mathrm{x})$ = a posteriori probabilities (as provided by the trained model) \n",
    "- $\\hat{p}(\\omega_{i})$ = the new priors\n",
    "- $\\hat{p}_{t}(\\omega_{i})$ = old priors\n",
    "\n",
    "The denominator ensures that the corrected a posteriori probabilities sum to one.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_from_dir_ending_with(directory, ending, without_file_extension = False):\n",
    "    file_list = []\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(key=lambda fn: os.path.getmtime(os.path.join(directory, fn))) # sort by date\n",
    "    for file in files:\n",
    "        if file.endswith(ending):\n",
    "            if without_file_extension:\n",
    "                file_list.append(os.path.splitext(file)[0])\n",
    "            else:\n",
    "                file_list.append(file)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_and_test_a_model_on_a_mnist_subset(mnist_subset, ckpt_dir, ckpt_filemame, plot_filename):\n",
    "    print('Restoring model from {}{}'.format(ckpt_dir, ckpt_filemame))\n",
    "    restored_distr_pos = utils.restore_variable_from_checkpoint(ckpt_dir=ckpt_dir, ckpt_file=ckpt_filemame, var_name = 'distr_pos')\n",
    "    if restored_distr_pos is None:\n",
    "        restored_distr_pos = [False, False, False, False, False]\n",
    "    restored_model = Lenet5WithDistr(mnist_dataset=mnist_ds, verbose=False, distr_pos=restored_distr_pos)\n",
    "    restored_model.restore_session(ckpt_dir=ckpt_dir, ckpt_filename=ckpt_filemame)\n",
    "    train_distr = restored_model.session.run(restored_model.train_distr)\n",
    "    test_loss, test_acc, total_predict, total_actual, wrong_predict_images, total_softmax_output_probs = restored_model.test_data(mnist_subset.test)\n",
    "\n",
    "    print('test_loss = {:.4f}, test_acc = {:.4f} ({}/{})'.format(test_loss, test_acc, mnist_subset.test.num_examples - len(wrong_predict_images), mnist_subset.test.num_examples))\n",
    "    \n",
    "    # sort wrong_predict_images by target label\n",
    "    correct_predict = total_predict[total_actual == total_predict]\n",
    "    wrong_predict = total_predict[total_actual != total_predict]\n",
    "    wrong_predict_softmax_output_probs = total_softmax_output_probs[total_actual != total_predict]\n",
    "    wrong_actual = total_actual[total_actual != total_predict]\n",
    "    wrong_predict_images = np.array(wrong_predict_images)\n",
    "    wrong_predict_images_sorted = wrong_predict_images[wrong_actual.argsort(), ]\n",
    "    wrong_predict_images_sorted = [image for image in wrong_predict_images_sorted]\n",
    "\n",
    "    count_figures = 6\n",
    "    fig = plt.figure(figsize=(30, 3))\n",
    "    fig.suptitle(y = 1.1, t = 'test_acc = {:.4f} ({}/{})'.format(test_acc, mnist_subset.test.num_examples - len(wrong_predict_images), mnist_subset.test.num_examples), fontsize=18, fontweight='bold')\n",
    "\n",
    "    k = 1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(10), train_distr)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('train label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(10), mnist_subset.test.label_distr)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('test label distr')\n",
    "\n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(correct_predict, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('correct predicted label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(wrong_predict, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('wrong predicted label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(wrong_actual, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('wrong actual label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(0, 10), np.average(wrong_predict_softmax_output_probs, axis=0))\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('average of wrong actual softmax output probabilities')\n",
    "\n",
    "    plt.savefig(os.path.join(ckpt_dir, plot_filename))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - test and save the results to file\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK_DIRS = [\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/150samples/',\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/250samples/',\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/500samples/',\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/1000samples/',\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/5000samples/',\n",
    "#     './results/PriorProbabilityShift_experiment_6_[]samples/10000samples/',\n",
    "# ] \n",
    "\n",
    "WORK_DIRS = [\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/150samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/250samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/500samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/1000samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/5000samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/10000samples/',\n",
    "]  \n",
    "\n",
    "NO_TESTS_PER_MODEL = 50\n",
    "TEST_SUBSET_SIZE = 3000\n",
    "SAVE_DICT_TO_FILE = True\n",
    "USE_SHORTCUT_METHOD_FOR_TESTING = True\n",
    "USE_NORMAL_METHOD_FOR_TESTING = False\n",
    "ADJUSTING_USING_REAL_PRIORS = True\n",
    "\n",
    "def save_test_results_to_dict(test_dict, test_method, test_size, id_test, id_distr, test_distr, test_acc, total_actual, total_predict):\n",
    "    test_dict['test_method'].append(test_method)\n",
    "    test_dict['testset_size'].append(test_size)\n",
    "    test_dict['id_test'].append(id_test)\n",
    "    test_dict['id_distr'].append(id_distr)\n",
    "    test_dict['test_distr'].append(test_distr)\n",
    "    test_dict['test_acc'].append(test_acc)\n",
    "    correct_predict = total_predict[total_actual == total_predict]\n",
    "    wrong_predict = total_predict[total_actual != total_predict]\n",
    "    wrong_actual = total_actual[total_actual != total_predict]\n",
    "    test_dict['correct_predicted_counts'].append(np.bincount(correct_predict, minlength=10))\n",
    "    test_dict['wrong_predicted_counts'].append(np.bincount(wrong_predict, minlength=10))\n",
    "    test_dict['wrong_actual_counts'].append(np.bincount(wrong_actual, minlength=10))\n",
    "        \n",
    "\n",
    "for WORK_DIR in WORK_DIRS:\n",
    "    ckpt_file_list = utils.get_all_files_from_dir_ending_with(WORK_DIR, \"ckpt.meta\", without_file_extension=True)\n",
    "\n",
    "    # dictionary used for saving all the results\n",
    "    full_results_dict = {}\n",
    "\n",
    "    for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "        print('{}: Restoring model {} from {}{}'.format(utils.now_as_str(), idx_model, WORK_DIR, ckpt_file))\n",
    "\n",
    "        test_model = Lenet5(mnist_dataset=mnist_ds, display_summary=False)\n",
    "        test_model.restore_session(ckpt_dir=WORK_DIR, ckpt_filename=ckpt_file)\n",
    "        current_model_train_distr = test_model.session.run(test_model.train_distr)\n",
    "        full_results_dict[ckpt_file] = {'idx_model':idx_model, 'train_distr': current_model_train_distr, \n",
    "                                       'test_results': {'test_method':[], 'testset_size':[], 'id_test':[], 'id_distr':[], 'test_distr':[], 'test_acc':[], 'correct_predicted_counts':[], 'wrong_predicted_counts':[], 'wrong_actual_counts':[]}}\n",
    "\n",
    "        print('The restored model {} was trained using distr: {}\\n'.format(idx_model, current_model_train_distr))\n",
    "\n",
    "        # test on the entire MNIST dataset\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "        print('Testing model on the entire dataset (distr. = {})'.format(mnist_ds.test.label_distr))\n",
    "        test_loss, test_acc, total_predict, total_actual, wrong_predict_images, output_probs = test_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "        print('test_acc = {:.3f}% ({}/{})\\n'.format(test_acc * 100, mnist_ds.test.num_examples - len(wrong_predict_images), mnist_ds.test.num_examples))\n",
    "\n",
    "        # save the test results\n",
    "        save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='entire', test_size=mnist_ds.test.num_examples, id_test=-1, id_distr=-1, \n",
    "                                  test_distr=mnist_ds.test.label_distr, test_acc=test_acc, total_actual=total_actual, total_predict=total_predict)\n",
    "\n",
    "        print('{}: start testing model on distributions\\n'.format(utils.now_as_str()))\n",
    "        for idx_distr, distr in enumerate(distrs_used_for_training):\n",
    "            print('Testing model {} ({} times) on distribution {}: {}'.format(idx_model, NO_TESTS_PER_MODEL, idx_distr, distr))\n",
    "\n",
    "            # compute theoretical mean accuracy\n",
    "            wrong_actual = total_actual[total_actual != total_predict]\n",
    "            theoretical_wrong_count = np.sum(np.bincount(wrong_actual, minlength=10) * (10 * distr)) * (TEST_SUBSET_SIZE / mnist_ds.test.num_examples)\n",
    "            print('theoretical \\u03BC(acc) = {:.3f}% ({}/{})'.format((1 - theoretical_wrong_count / TEST_SUBSET_SIZE) * 100, TEST_SUBSET_SIZE - theoretical_wrong_count, TEST_SUBSET_SIZE))\n",
    "\n",
    "            if USE_SHORTCUT_METHOD_FOR_TESTING:\n",
    "                list_acc_shortcut_method = []\n",
    "                list_acc_shortcut_method_adj = []\n",
    "                for id_test in range(NO_TESTS_PER_MODEL):\n",
    "                    indices_wrt_distr = utils.get_indices_wrt_distr(labels=total_actual, weights=distr, max_no_examples=TEST_SUBSET_SIZE)\n",
    "                    num_examples_selection = len(indices_wrt_distr)\n",
    "                    total_predict_selection = total_predict[indices_wrt_distr]\n",
    "                    total_actual_selection = total_actual[indices_wrt_distr]\n",
    "                    output_probs_selection = output_probs[indices_wrt_distr]\n",
    "                    no_correct_predicted = np.sum(total_predict_selection == total_actual_selection)\n",
    "                    test_acc = no_correct_predicted / num_examples_selection\n",
    "    #                 print('id_test = {} : test_acc = {:.3f}% ({}/{})4'.format(id_test, test_acc * 100, no_correct_predicted, num_examples_selection))\n",
    "\n",
    "                    list_acc_shortcut_method.append(test_acc)\n",
    "\n",
    "                    # save the test results\n",
    "                    save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='shortcut', test_size=num_examples_selection, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=test_acc, total_actual=total_actual_selection, total_predict=total_predict_selection)\n",
    "\n",
    "                    if ADJUSTING_USING_REAL_PRIORS:\n",
    "                        # adjust the outputs\n",
    "                        real_priors = np.bincount(total_actual_selection, minlength=10) / num_examples_selection\n",
    "                        old_priors = current_model_train_distr\n",
    "    #                     print('real priors = {}'.format(real_priors))\n",
    "    #                     print('old prios   = {}'.format(old_priors))\n",
    "                        adj_output_probs_selection = (real_priors / old_priors) * output_probs_selection / (np.sum((real_priors / old_priors) * output_probs_selection, axis = 1)[:, None])\n",
    "                        adj_total_predict_selection = np.argmax(adj_output_probs_selection, axis=1)\n",
    "                        count_correct_predicted = np.sum(total_predict_selection == total_actual_selection)\n",
    "    #                     print('Old accuracy: {:.3f} ({}/{})'.format(count_correct_predicted / num_examples_selection, count_correct_predicted, num_examples_selection))\n",
    "                        count_correct_predicted = np.sum(adj_total_predict_selection == total_actual_selection)\n",
    "    #                     print('New accuracy: {:.3f} ({}/{})'.format(count_correct_predicted / num_examples_selection, count_correct_predicted, num_examples_selection))\n",
    "\n",
    "                        # save the results into dictionary\n",
    "                        adj_test_acc = np.sum(adj_total_predict_selection == total_actual_selection) / len(total_predict_selection)\n",
    "                        list_acc_shortcut_method_adj.append(adj_test_acc)\n",
    "                        save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='shortcut_adj', test_size=num_examples_selection, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=adj_test_acc, total_actual=total_actual_selection, total_predict=adj_total_predict_selection)\n",
    "\n",
    "\n",
    "                print('\\u03BC(acc_s ) = {:.3f}%, \\u03C3(acc_s ) = {:.3f}%'.format(np.mean(list_acc_shortcut_method) * 100, np.std(list_acc_shortcut_method, ddof=1) * 100))\n",
    "                print('\\u03BC(adj_acc_s) = {:.3f}%, \\u03C3(adj_acc_s) = {:.3f}%'.format(np.mean(list_acc_shortcut_method_adj) * 100, np.std(list_acc_shortcut_method_adj, ddof=1) * 100))\n",
    "\n",
    "\n",
    "            if USE_NORMAL_METHOD_FOR_TESTING:\n",
    "                list_acc_normal_method = []\n",
    "                list_acc_normal_method_adj = []\n",
    "                for id_test in range(NO_TESTS_PER_MODEL):\n",
    "                    mnist_ds1 = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "                    for s in range(id_test):\n",
    "                        mnist_ds1.test.shuffle() # when MNISTDataset is instanced, the rg is reseted, so explicitly shuffling is needed for getting different results\n",
    "                    mnist_ds1.impose_distribution(weights=distr, max_test_size=TEST_SUBSET_SIZE)\n",
    "                    test_loss1, test_acc1, total_predict1, total_actual1, wrong_predict_images1, output_probs1 = test_model.test_data(mnist_ds1.test, use_only_one_batch=True)\n",
    "    #                 print('id_test = {} : test_acc1 = {:.3f}% ({}/{})'.format(id_test, test_acc1 * 100, mnist_ds1.test.num_examples - len(wrong_predict_images1), mnist_ds1.test.num_examples))\n",
    "\n",
    "                    list_acc_normal_method.append(test_acc1)\n",
    "\n",
    "                    # save the test results\n",
    "                    save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='normal', test_size=mnist_ds1.test.num_examples, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=test_acc1, total_actual=total_actual1, total_predict=total_predict1)\n",
    "\n",
    "                    if ADJUSTING_USING_REAL_PRIORS:\n",
    "                        # adjust the outputs\n",
    "                        real_priors = mnist_ds1.test.label_distr\n",
    "                        old_priors = current_model_train_distr\n",
    "                        adj_output_probs1 = (real_priors / old_priors) * output_probs1 / (np.sum((real_priors / old_priors) * output_probs1, axis = 1)[:, None])\n",
    "                        adj_total_predict1 = np.argmax(adj_output_probs1, axis=1)\n",
    "                        count_correct_predicted = np.sum(total_predict1 == total_actual1)\n",
    "    #                     print('Old accuracy: {:.3f} ({}/{})'.format(count_correct_predicted / mnist_ds1.test.num_examples, count_correct_predicted, mnist_ds1.test.num_examples))\n",
    "                        count_correct_predicted = np.sum(adj_total_predict1 == total_actual1)\n",
    "    #                     print('New accuracy: {:.3f} ({}/{})'.format(count_correct_predicted / mnist_ds1.test.num_examples, count_correct_predicted, mnist_ds1.test.num_examples))\n",
    "\n",
    "                        # save the results into dictionary\n",
    "                        adj_test_acc = np.sum(adj_total_predict1 == total_actual1) / mnist_ds1.test.num_examples\n",
    "                        list_acc_normal_method_adj.append(adj_test_acc)\n",
    "                        save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='normal_adj', test_size=mnist_ds1.test.num_examples, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=adj_test_acc, total_actual=total_actual1, total_predict=adj_total_predict1)\n",
    "\n",
    "                print('\\u03BC(acc_n) = {:.3f}%, \\u03C3(acc_n) = {:.3f}%'.format(np.mean(list_acc_normal_method) * 100, np.std(list_acc_normal_method, ddof=1) * 100))\n",
    "                print('\\u03BC(adj_acc_n) = {:.3f}%, \\u03C3(adj_acc_n) = {:.3f}%'.format(np.mean(list_acc_normal_method_adj) * 100, np.std(list_acc_normal_method_adj, ddof=1) * 100))\n",
    "\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "        # analyze error distribution\n",
    "        # restore_and_test_a_model_on_a_mnist_subset(mnist_ds, ckpt_dir=WORK_DIR, ckpt_filemame=ckpt_file, plot_filename = 'test_model')\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "\n",
    "    # save the above results dictionary to file\n",
    "    if SAVE_DICT_TO_FILE:\n",
    "        filename = 'testing_results_{}testsPerModel.dict.pickle'.format(NO_TESTS_PER_MODEL)\n",
    "        full_filepath = os.path.join(WORK_DIR, filename)\n",
    "        filehandler = open(full_filepath, 'wb') \n",
    "        pickle.dump(full_results_dict, filehandler)\n",
    "        print('Results dictionary was succesfully saved to: {}'.format(full_filepath))\n",
    "        filehandler.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - restore the results saved earlier and plot them\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRS = [\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/150samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/250samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/500samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/1000samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/5000samples/',\n",
    "    './results/PriorProbabilityShift_experiment_6_[]samples_2/10000samples/',\n",
    "]  \n",
    "filename = 'testing_results_50testsPerModel.dict.pickle'\n",
    "\n",
    "for WORK_DIR in WORK_DIRS:\n",
    "\n",
    "    filehandler = open(os.path.join(WORK_DIR,filename), 'rb') \n",
    "    restored_perf_dict = pickle.load(filehandler)\n",
    "    filehandler.close()\n",
    "    print('Results dictionary was succesfully restored from: {}{}'.format(WORK_DIR, filename))\n",
    "\n",
    "    # get all distributions used for training\n",
    "    distrs_used_for_training = []\n",
    "    for ckpt_file in restored_perf_dict.keys():\n",
    "        distrs_used_for_training.append(restored_perf_dict[ckpt_file]['train_distr'])\n",
    "    L = len(distrs_used_for_training)\n",
    "\n",
    "    # build the average accuracy matrices and distribution matrices, assuming that each model was tested on all distributions from distrs_used_for_training\n",
    "    avg_acc_matrix = np.empty((L, L))\n",
    "    avg_acc_matrix_adj = np.empty((L, L))\n",
    "    std_matrix = np.empty((L, L))\n",
    "    std_matrix_adj = np.empty((L, L))\n",
    "\n",
    "    avg_correct_predictions_distr_matrix = np.empty((L, L, 10))\n",
    "    avg_correct_predictions_distr_matrix_adj = np.empty((L, L, 10))\n",
    "    avg_wrong_predictions_distr_matrix = np.empty((L, L, 10))\n",
    "    std_wrong_predictions_distr_matrix = np.empty((L, L, 10))\n",
    "    avg_wrong_predictions_distr_matrix_adj = np.empty((L, L, 10))\n",
    "    avg_wrong_actual_predictions_distr_matrix = np.empty((L, L, 10))\n",
    "    avg_wrong_actual_predictions_distr_matrix_adj = np.empty((L, L, 10))\n",
    "\n",
    "    def std_1ddof(x):\n",
    "        return np.std(x, ddof=1)\n",
    "\n",
    "    for id_model, ckpt_file in enumerate(restored_perf_dict.keys()):\n",
    "\n",
    "    #     build a pandas dataframe from test results dictionary\n",
    "        perf_df = pd.DataFrame(restored_perf_dict[ckpt_file]['test_results'], columns=list(restored_perf_dict[ckpt_file]['test_results'].keys()))\n",
    "        temp_df = perf_df[perf_df['test_method'] == 'shortcut'].groupby(['id_distr']).agg([np.mean, std_1ddof])\n",
    "        avg_acc_matrix[id_model, :] = temp_df['test_acc']['mean']\n",
    "        std_matrix[id_model, :] = temp_df['test_acc']['std_1ddof']\n",
    "\n",
    "        temp_df = perf_df[perf_df['test_method'] == 'shortcut_adj'].groupby(['id_distr']).agg([np.mean, std_1ddof])\n",
    "        avg_acc_matrix_adj[id_model, :] = temp_df['test_acc']['mean']\n",
    "        std_matrix_adj[id_model, :] = temp_df['test_acc']['std_1ddof']\n",
    "\n",
    "        for t in range(L):\n",
    "            avg_correct_predictions_distr_matrix[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut') & (perf_df['id_distr'] == t)]['correct_predicted_counts'], axis=0)\n",
    "            avg_wrong_predictions_distr_matrix[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut') & (perf_df['id_distr'] == t)]['wrong_predicted_counts'], axis=0)\n",
    "    #         std_wrong_predictions_distr_matrix[id_model, t, :] = np.std(np.array(perf_df[(perf_df['test_method'] == 'shortcut') & (perf_df['id_distr'] == t)]['wrong_predicted_counts']), axis=0, ddof=1)\n",
    "            avg_wrong_actual_predictions_distr_matrix[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut') & (perf_df['id_distr'] == t)]['wrong_actual_counts'], axis=0)\n",
    "            avg_correct_predictions_distr_matrix_adj[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut_adj') & (perf_df['id_distr'] == t)]['correct_predicted_counts'], axis=0)\n",
    "            avg_wrong_predictions_distr_matrix_adj[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut_adj') & (perf_df['id_distr'] == t)]['wrong_predicted_counts'], axis=0)\n",
    "            avg_wrong_actual_predictions_distr_matrix_adj[id_model, t, :] = np.average(perf_df[(perf_df['test_method'] == 'shortcut_adj') & (perf_df['id_distr'] == t)]['wrong_actual_counts'], axis=0)\n",
    "\n",
    "    diff_acc_matrix = avg_acc_matrix_adj - avg_acc_matrix\n",
    "\n",
    "    # plot the average accuracy comparison matrix\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix, use_percent_for_accuracies=True,\n",
    "                                           std_matrix=std_matrix, title='accuracy matrix (avg and std)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.acc_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot distributions corresponding to correct predictions\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix, distr_matrix=avg_correct_predictions_distr_matrix,\n",
    "                                           use_percent_for_accuracies=True, title='correct predictions distr. (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.correct_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot distributions corresponding to wrong predictions\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix, distr_matrix=avg_wrong_predictions_distr_matrix,\n",
    "                                           use_percent_for_accuracies=True, title='wrong predictions (predicted) distr. (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.wrong_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot distributions corresponding to wrong actual predictions\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix, distr_matrix=avg_wrong_actual_predictions_distr_matrix,\n",
    "                                           use_percent_for_accuracies=True, title='wrong predictions (actual) distr. (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.wrong_actual_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot the average accuracy comparison matrix (adjusted version using real priors)\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix_adj, use_percent_for_accuracies=True,\n",
    "                                           std_matrix=std_matrix_adj, title='accuracy matrix after adj. using real priors (avg and std)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.adj_acc_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # build and plot distributions corresponding to correct predictions (adjusted version using real priors)\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix_adj, distr_matrix=avg_correct_predictions_distr_matrix_adj,\n",
    "                                           use_percent_for_accuracies=True, title='correct predictions distr. after adj. using real priors (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.adj_correct_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot distributions corresponding to wrong predictions (adjusted version using real priors)\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix_adj, distr_matrix=avg_wrong_predictions_distr_matrix_adj,\n",
    "                                           use_percent_for_accuracies=True, title='wrong predictions (predicted) distr. after adj. using real priors (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.adj_wrong_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot distributions corresponding to wrong actual predictions (adjusted version using real priors)\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix_adj, distr_matrix=avg_wrong_actual_predictions_distr_matrix_adj,\n",
    "                                           use_percent_for_accuracies=True, title='wrong predictions (actual) distr. after adj. using real priors (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,filename + '.adj_wrong_actual_predictions_distr_matrix.png'), bbox_inches='tight', pad_inches=1)\n",
    "\n",
    "    # plot the difference of accuracy matrix (standard version vs adjusted version using real priors)\n",
    "    acc_matrix_plt = utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=diff_acc_matrix, use_percent_for_accuracies=True,\n",
    "                                          title='diff. accuracy matrix (before vs. after adj. using real priors) (avg)')\n",
    "    acc_matrix_plt.savefig(os.path.join(WORK_DIR,'{}_{:.1f}%avgDiff.diff_acc_matrix.png'.format(filename, np.average(diff_acc_matrix) * 100)), bbox_inches='tight', pad_inches=1)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - test the models and the adjusting method on the entire MNIST testset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_10000samples/\"\n",
    "# WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_5000samples/\"\n",
    "WORK_DIR = \"./results/PriorProbabilityShift_experiment_6_[]samples_2/1000samples/\"\n",
    "\n",
    "ckpt_file_list = utils.get_all_files_from_dir_ending_with(WORK_DIR, \"ckpt.meta\", without_file_extension=True)\n",
    "\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    print('Restoring model {}{} {}'.format(WORK_DIR, idx_model, ckpt_file))\n",
    "    test_model = Lenet5(mnist_dataset=mnist_ds, display_summary=False)\n",
    "    test_model.restore_session(ckpt_dir=WORK_DIR, ckpt_filename=ckpt_file)\n",
    "    test_model_train_distr = test_model.session.run(test_model.train_distr)\n",
    "    print('The restored model was trained using distr: {}\\n'.format(test_model_train_distr))\n",
    "    \n",
    "    # test it on the entire MNIST test set\n",
    "    mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "    test_loss, test_acc, total_predict, total_actual, wrong_predict_images, output_probs = test_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "  \n",
    "    # analyze error distribution\n",
    "    restore_and_test_a_model_on_a_mnist_subset(mnist_ds, ckpt_dir=WORK_DIR, ckpt_filemame=ckpt_file, plot_filename = 'test_model')\n",
    "    \n",
    "    # adjusing the ouputs using real priots\n",
    "    real_priors = mnist_ds.test.label_distr\n",
    "    old_priors = test_model_train_distr\n",
    "    print('real priors = {}'.format(real_priors))\n",
    "    print('old prios   = {}'.format(old_priors))\n",
    "\n",
    "    corrected_output_probs = (real_priors / old_priors) * output_probs / (np.sum((real_priors / old_priors) * output_probs, axis = 1)[:, None])\n",
    "    corrected_total_predict = np.argmax(corrected_output_probs, axis=1)\n",
    "    \n",
    "    count_correct_predicted = np.sum(total_predict == total_actual)\n",
    "    print('Old accuracy: {:.3f} ({}/{})'.format(count_correct_predicted / mnist_ds.test.num_examples, count_correct_predicted, mnist_ds.test.num_examples))\n",
    "    count_correct_predicted = np.sum(corrected_total_predict == total_actual)\n",
    "    print('New accuracy: {:.3f} ({}/{}), after adjusted using real priors'.format(count_correct_predicted / mnist_ds.test.num_examples, count_correct_predicted, mnist_ds.test.num_examples))\n",
    "    \n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - analyze some examples for which the adjusting using the real priors changed the decision\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_IMAGES_TO_DISPLAY_PER_MODEL = 3\n",
    "# WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_10000samples/\"\n",
    "# WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_5000samples/\"\n",
    "WORK_DIR = \"./results/PriorProbabilityShift_experiment_6_[]samples_2/1000samples/\"\n",
    "\n",
    "ckpt_file_list = utils.get_all_files_from_dir_ending_with(WORK_DIR, \"ckpt.meta\", without_file_extension=True)\n",
    "\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    print('Restoring model {}{} {}'.format(WORK_DIR, idx_model, ckpt_file))\n",
    "    test_model = Lenet5(mnist_dataset=mnist_ds, display_summary=False)\n",
    "    test_model.restore_session(ckpt_dir=WORK_DIR, ckpt_filename=ckpt_file)\n",
    "    test_model_train_distr = test_model.session.run(test_model.train_distr)\n",
    "#     print('The restored model was trained using distr: {}\\n'.format(test_model_train_distr))\n",
    "    \n",
    "    for idx_distr, distr in enumerate(distrs_used_for_training):\n",
    "        if (test_model_train_distr == distr).all():\n",
    "            continue\n",
    "#         print('Train distr: {}'.format(test_model_train_distr))\n",
    "#         print('Test distr: {}'.format(distr))\n",
    "        \n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "        mnist_ds.impose_distribution(weights=distr, global_max_weight=np.max(distrs_used_for_training))\n",
    "        test_loss, test_acc, total_predict, total_actual, wrong_predict_images, output_probs = test_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "        correct_predict = total_predict[total_actual == total_predict]\n",
    "        wrong_predict = total_predict[total_actual != total_predict]\n",
    "        wrong_actual = total_actual[total_actual != total_predict]\n",
    "\n",
    "        # adjust the outputs\n",
    "        real_priors = mnist_ds.test.label_distr\n",
    "        old_priors = test_model_train_distr\n",
    "#         print('real priors = {}'.format(real_priors))\n",
    "#         print('old prios   = {}'.format(old_priors))\n",
    "        adj_output_probs = (real_priors / old_priors) * output_probs / (np.sum((real_priors / old_priors) * output_probs, axis = 1)[:, None])\n",
    "        adj_total_predict = np.argmax(adj_output_probs, axis=1).astype(np.int32)\n",
    "        count_correct_predicted = np.sum(total_predict == total_actual)\n",
    "        acc = count_correct_predicted / mnist_ds.test.num_examples\n",
    "        print('Old accuracy: {:.1f}% ({}/{})'.format(acc * 100, count_correct_predicted, mnist_ds.test.num_examples))\n",
    "        count_correct_predicted = np.sum(adj_total_predict == total_actual)\n",
    "        \n",
    "        adj_test_acc = np.sum(adj_total_predict == total_actual) / len(adj_total_predict)\n",
    "        print('New accuracy: {:.1f}%({}/{}), diff = {:.2f}%'.format(adj_test_acc * 100, count_correct_predicted, mnist_ds.test.num_examples,(adj_test_acc - acc) * 100))\n",
    "        \n",
    "        adj_correct_predict = adj_total_predict[total_actual == adj_total_predict]\n",
    "        adj_wrong_predict = adj_total_predict[total_actual != adj_total_predict]\n",
    "        adj_wrong_actual = total_actual[total_actual != adj_total_predict]\n",
    "        \n",
    "        indices_wrong_predicted_corrected_by_adj = np.where(np.logical_and((total_actual != total_predict), (total_actual == adj_total_predict)) == True)[0]\n",
    "        indices_correct_predicted_disturbed_by_adj = np.where(np.logical_and((total_actual == total_predict), (total_actual != adj_total_predict)) == True)[0]\n",
    "        print('#wrong_predicted_corrected_by_adj = {}'.format(indices_wrong_predicted_corrected_by_adj.shape[0]))\n",
    "        print('#correct_predicted_disturbed_by_adj = {}'.format(indices_correct_predicted_disturbed_by_adj.shape[0]))\n",
    "        if indices_wrong_predicted_corrected_by_adj.shape[0] > MAX_IMAGES_TO_DISPLAY_PER_MODEL:\n",
    "            indices_wrong_predicted_corrected_by_adj = indices_wrong_predicted_corrected_by_adj[0:MAX_IMAGES_TO_DISPLAY_PER_MODEL]\n",
    "        if indices_correct_predicted_disturbed_by_adj.shape[0] > MAX_IMAGES_TO_DISPLAY_PER_MODEL:\n",
    "            indices_correct_predicted_disturbed_by_adj = indices_correct_predicted_disturbed_by_adj[0:MAX_IMAGES_TO_DISPLAY_PER_MODEL]\n",
    "        for idx in np.concatenate((indices_wrong_predicted_corrected_by_adj, indices_correct_predicted_disturbed_by_adj)):\n",
    "#             print('output probs: {}'.format(output_probs[idx]))\n",
    "#             print('adj. probs: {}'.format(adj_output_probs[idx]))\n",
    "\n",
    "            count_figures = 5\n",
    "            fig = plt.figure(figsize=(20, 2))\n",
    "\n",
    "            k = 1\n",
    "            plt.subplot(1, count_figures, k)\n",
    "            plt.bar(range(10), old_priors)\n",
    "            plt.xticks(range(0, 10))\n",
    "            plt.title('old priors (= train label distr)')\n",
    "\n",
    "            k+=1\n",
    "            plt.subplot(1, count_figures, k)\n",
    "            plt.bar(range(10), real_priors)\n",
    "            plt.xticks(range(0, 10))\n",
    "            plt.title('real priors (= test label distr)')\n",
    "\n",
    "            k += 1\n",
    "            plt.subplot(1, count_figures, k)\n",
    "            plt.imshow(mnist_ds.test.images[idx].reshape((28,28)), cmap='gray')\n",
    "            plt.title('initial predicted {} and is {}; (after adj.: {})'.format(total_predict[idx], total_actual[idx], adj_total_predict[idx]))\n",
    "\n",
    "            k += 1\n",
    "            plt.subplot(1, count_figures, k)\n",
    "            plt.bar(range(10), output_probs[idx])\n",
    "            plt.xticks(range(0, 10))\n",
    "            plt.title('output probs')\n",
    "            axes = plt.gca()\n",
    "            axes.set_ylim([0,1])\n",
    "\n",
    "            k+=1\n",
    "            plt.subplot(1, count_figures, k)\n",
    "            plt.bar(range(10), adj_output_probs[idx])\n",
    "            plt.xticks(range(0, 10))\n",
    "            plt.title('adj. probs')\n",
    "            axes = plt.gca()\n",
    "            axes.set_ylim([0,1])\n",
    "\n",
    "            plt.show()\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO. Adaptation using estimated priors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
