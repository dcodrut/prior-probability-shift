{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Local Adaptive Model - Notebook 2\n",
    "\n",
    "* Prior Probability Shift is one of the common problems encountered in Machine Learning algorithms.   \n",
    "* There are some approaches for dealing with this problem in a 'static' scenario. But there are situations in which we need a model which deals with secvential data as input (e.g. a server which gets input from different users, with different data distributions).   \n",
    "* In this project, we try to build a model which self adapts its predictions based on the local label distribution. \n",
    "\n",
    "### About notebook 2\n",
    "\n",
    "In this notebook we ilustrate an example of how a different test distribution has an impact on the model's performance.\n",
    "We train multiple models, on a range of subsets of MNIST, with different distributions. Then each model is tested on a range of test subsets with respect to the distributions considered in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Notebook setup and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "# %matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "import pickle\n",
    "from training_plotter import TrainingPlotter\n",
    "from dataset import MNISTDataset\n",
    "from utils import Utils\n",
    "from lenet5 import Lenet5\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(linewidth = 150)\n",
    "np.set_printoptions(edgeitems = 10)\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random generator using a constant seed in order to reproduce results\n",
    "seed = 112358\n",
    "nprg = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_TRAIN_IMAGES_FILEPATH = 'MNIST_dataset/train-images.idx3-ubyte'\n",
    "MNIST_TRAIN_LABELS_FILEPATH = 'MNIST_dataset/train-labels.idx1-ubyte'\n",
    "MNIST_TEST_IMAGES_FILEPATH = 'MNIST_dataset/t10k-images.idx3-ubyte'\n",
    "MNIST_TEST_LABELS_FILEPATH = 'MNIST_dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a subset of  MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_subset = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "subset_size = 10000\n",
    "mnist_subset.impose_distr_on_train_dataset(subset_size=subset_size, weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "print(np.sum(mnist_subset.train.images))\n",
    "print(mnist_subset.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds_aug = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "mnist_ds_aug.enhance_with_random_rotate(ratio = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_ds_aug.enhance_with_random_zoomin(ratio = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds_aug.enhance_with_random_zoomin_and_rotate(ratio = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Train and test models on subsets with different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering some subsets with different class distributions, we want to use the same amount of training data\n",
    "\n",
    "We will train 10 * num_distributions_considered as follows:\n",
    "- consider a set of n distributions used for generating subsets of original dataset:  \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\{weights_1,weights_2,...,weights_n\\}\n",
    "\\text{, where } \n",
    "weights_k = \\{weight_{class_0},weight_{class_1},...,weight_{class_9}\\}\n",
    "\\end{equation}    \n",
    "\n",
    "- than, for each distribution we'll consider another 9 distributions by circular shifting the original one:\n",
    "\\begin{equation}\n",
    "\\text{For the } k^{th} \\text{ distribution, we'll also consider:}\\\\\n",
    "\\{weight_{class_1},weight_{class_2},...,weight_{class_9},weight_{class_0}\\}, \n",
    "\\{weight_{class_2},weight_{class_3},...,weight_{class_0},weight_{class_1}\\}, \n",
    "\\dots, \n",
    "\\{weight_{class_9},weight_{class_0},...,weight_{class_7},weight_{class_8}\\} \\\\\n",
    "\\end{equation}    \n",
    "\n",
    "In order to keep the number of examples constant for every distributions considered, we take into account that the 'worst' case (i.e. when we have to use the lowest number of examples) happens when the lowest weight value (from all distributions considered) will correspond to the smallest bin =>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Considering }\\ count_{min} = min\\{{counts_{class_i}}|\\  i = \\overline{0,9}\\} \\text{ and }\\\\\n",
    "\\ weight_{max} =  max\\{{weight_{class_i}} \\ of \\ weights_k \\ |\\  i = \\overline{0,9}, k = \\overline{1,n} \\} \\ \\  \\text{(i.e. maximum weight from all n sets of weights)} \\\\\n",
    "num\\_examples = \\sum_{i=0}^{i=9} count_{min}* weight_{class_i} * \\frac{1}{weight_{max}} =  \\frac{counts_{min}}{weight_{max}} \\sum_{i=0}^{i=9} weight_{class_i} = \\frac{counts_{min}}{weight_{max}} \\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build some label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate some distributions using geometric progressions\n",
    "ratios = [1,1.05,1.1,1.2,1.4,2,4]\n",
    "perms_per_ratio = 5\n",
    "generated_distrs = []\n",
    "for idx, ratio in enumerate(ratios):\n",
    "    distr = [0.1]\n",
    "    for i in range(9):\n",
    "        distr.append(distr[i] * ratio)\n",
    "    distr = np.array(distr)\n",
    "    distr /= np.sum(distr)\n",
    "    print('generated_distr[{}]:'.format(idx))\n",
    "    print(', '.join([str(np.round(x, decimals=3)) for x in distr]))\n",
    "#     plt.bar(range(10),distr)\n",
    "#     plt.show()\n",
    "    generated_distrs.append(distr.tolist())\n",
    "    \n",
    "# Select some of the above distribution and build others by circular shifting\n",
    "base_distrs = np.array([generated_distrs[0],\n",
    "                       generated_distrs[2],\n",
    "                      generated_distrs[4]])\n",
    "num_distributions_considered = base_distrs.shape[0]\n",
    "print('\\nBase distributions considered:\\n', base_distrs)\n",
    "for distr in base_distrs:\n",
    "    print(distr)\n",
    "    plt.bar(range(10),distr)\n",
    "    plt.show()\n",
    "    \n",
    "# check the maximum number of examples that can be used using the above rule\n",
    "counts_per_label_training = np.bincount(np.argmax(mnist_ds.train.labels, axis=1))\n",
    "counts_per_label_test = np.bincount(np.argmax(mnist_ds.test.labels, axis=1))\n",
    "print('Counts for each digit (training): ', counts_per_label_training)\n",
    "print('Counts for each digit (test): ', counts_per_label_test)\n",
    "counts_min_training = np.min(counts_per_label_training)\n",
    "counts_min_test = np.min(counts_per_label_test)\n",
    "weight_max = np.max(base_distrs)\n",
    "max_num_examples_training = np.floor(counts_min_training / weight_max).astype(np.int32)\n",
    "max_num_examples_test = np.floor(counts_min_test / weight_max).astype(np.int32)\n",
    "print('max_num_examples for training = ', max_num_examples_training)\n",
    "print('max_num_examples for test = ', max_num_examples_test)\n",
    "# round to hundreds\n",
    "max_num_examples_training -= max_num_examples_training % 100\n",
    "max_num_examples_test -= max_num_examples_test % 100\n",
    "print('max_num_examples_training (rounded to thousands): ',max_num_examples_training)\n",
    "print('max_num_examples_test (rounded to thousands): ',max_num_examples_test)\n",
    "\n",
    "# build subsets w.r.t. a distribution, with max_num_examples\n",
    "distrs_used_for_training = []\n",
    "for base_distr in base_distrs:\n",
    "    distr = deque(base_distr)\n",
    "    for i in range(10):\n",
    "        print(distr)\n",
    "        distrs_used_for_training.append(distr.copy())\n",
    "        last_distr = distr.copy()\n",
    "        distr.rotate(1)\n",
    "        if np.sum(np.abs(np.array(last_distr) - np.array(distr))) < 1.e-5:\n",
    "            break\n",
    "print('#distributions used for training = {}'.format(len(distrs_used_for_training)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build some distributions by hand\n",
    "distrs_used_for_training = []\n",
    "\n",
    "# uniform distribution\n",
    "distr = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "# normal distribution centered about label 4-5\n",
    "r = 2\n",
    "distr = [r**1,r**2,r**3,r**4,r**5,r**5,r**4,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# skewed normal distribution centered about 2\n",
    "distr = [r**3,r**4,r**5,r**4.5,r**4,r**3.5,r**3,r**2.5,r**2,r**1.5]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# skwed normal distribution centered about 7\n",
    "distr = [r**1.5,r**2,r**2.5,r**3,r**3.5,r**4,r**4.5,r**5,r**4,r**3]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# bimodal normal distribution\n",
    "distr = [r**1,r**2,r**3,r**2,r**1,r**1,r**2,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# bimodal skewed normal distribution\n",
    "distr = [r**3.5,r**4,r**3,r**2,r**1,r**1,r**2,r**3,r**4,r**3.5]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "\n",
    "# exponential distribution\n",
    "r=1.4\n",
    "distr = [r**1,r**2,r**3,r**4,r**5,r**6,r**7,r**8,r**9,r**10]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "# exponential distribution\n",
    "r=1.4\n",
    "distr = [r**10,r**9,r**8,r**7,r**6,r**5,r**4,r**3,r**2,r**1]\n",
    "distrs_used_for_training.append(distr/np.sum(distr))\n",
    "\n",
    "print('#distributions used for training = {}'.format(len(distrs_used_for_training)))\n",
    "for idx, distr in enumerate(distrs_used_for_training):\n",
    "    print('idx = {}: distr = {}'.format(idx,distr))\n",
    "    plt.bar(range(10), distr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train LeNet5 models by imposing the considered distributions on the original MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_max_weight = np.max(distrs_used_for_training)\n",
    "mnist_ds.backup()\n",
    "for k, distr in enumerate(distrs_used_for_training):\n",
    "    print('\\n\\nk = {}: Imposed distribution: {}'.format(k, np.round(np.array(distr), decimals=3)))\n",
    "    mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "    mnist_ds.impose_distribution(np.array(distr), global_max_weight)\n",
    "    lenet5_model = Lenet5(mnist_ds, \"with_imposed_distr_{}_\".format(k), epochs=40, batch_size=256, variable_mean=0, variable_stddev=0.1, learning_rate=0.001, drop_out_keep_prob=0.5)\n",
    "    lenet5_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a subset of  MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_size = 10000\n",
    "global_max_weight = np.max(distrs_used_for_training)\n",
    "for k, distr in enumerate(distrs_used_for_training):\n",
    "    mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "#     mnist_ds.train.shuffle()\n",
    "    print('\\n\\nk = {}: Imposed distribution: {}'.format(k, np.round(np.array(distr), decimals=3)))\n",
    "    mnist_ds.impose_distribution(np.array(distr), global_max_weight, max_training_size=subset_size)\n",
    "    lenet5_model = Lenet5(mnist_ds, \"with_imposed_distr_{}_{}samples\".format(k, subset_size), epochs=40, batch_size=256, variable_mean=0, variable_stddev=0.1, learning_rate=0.001, drop_out_keep_prob=0.5)\n",
    "    lenet5_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE_LIST = [150, 250, 500, 1000, 5000, 10000]\n",
    "global_max_weight = np.max(distrs_used_for_training)\n",
    "\n",
    "for subset_size in SUBSET_SIZE_LIST:\n",
    "    for k, distr in enumerate(distrs_used_for_training):\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "        print('\\n\\nk = {}: Imposed distribution: {}'.format(k, np.round(np.array(distr), decimals=3)))\n",
    "        mnist_ds.impose_distribution(np.array(distr), global_max_weight, max_training_size=subset_size)\n",
    "        lenet5_model = Lenet5(mnist_ds, \"with_imposed_distr_{}_{}samples\".format(k, subset_size), epochs=40, batch_size=50, variable_mean=0, variable_stddev=0.1, learning_rate=0.001, drop_out_keep_prob=0.75)\n",
    "        lenet5_model.train()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test the train models again on their test data and on data which respects the other distributions\n",
    "\n",
    "In order to summarize the results, we will build a matrix, similar to a confusion matrix, as following:\n",
    "- take each trained model and test it again on his initial test set\n",
    "- then test each model on the data that respects, by turn, the other distributions considered\n",
    "- save these results into a dictionary and then to disk for reuse them later\n",
    "- finally, build a matrix where an element $m_{ij}$ represents the test accuracy of model $i$ evaluated on the subset with distribution $j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_from_dir_ending_with(directory, ending, without_file_extension = False):\n",
    "    file_list = []\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(key=lambda fn: os.path.getmtime(os.path.join(directory, fn))) # sort by date\n",
    "    for file in files:\n",
    "        if file.endswith(ending):\n",
    "            if without_file_extension:\n",
    "                file_list.append(os.path.splitext(file)[0])\n",
    "            else:\n",
    "                file_list.append(file)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ckpt_dir = \"./results/PriorProbabilityShift_experiment_5_augmented/\"\n",
    "ckpt_dir = \"./results/PriorProbabilityShift_experiment_5_10000samples/\"\n",
    "# ckpt_dir = \"./results/PriorProbabilityShift_experiment_5_5000samples/\"\n",
    "ckpt_file_list = get_all_files_from_dir_ending_with(ckpt_dir, \"ckpt.meta\", without_file_extension=True)\n",
    "perf_dict = {'idx_model':[], 'idx_distr':[], 'test_loss':[], 'test_acc':[], 'total_predict':[], 'total_actual':[], 'correct_predicted_distr':[], 'wrong_predicted_distr':[], 'wrong_actual_distr':[], 'train_distr':[], 'test_distr':[], 'ckpt_file':[]}\n",
    "\n",
    "# build a list with all ditributions considered in training phase\n",
    "distrs_used_for_training = []\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    print('Restoring model {} from {}'.format(idx_model, ckpt_file))\n",
    "    temp_model = Lenet5(mnist_dataset=mnist_ds, display_summary=False)\n",
    "    temp_model.restore_session(ckpt_dir=ckpt_dir, ckpt_filename=ckpt_file)\n",
    "    current_model_train_distr = temp_model.session.run(temp_model.train_distr)\n",
    "    distrs_used_for_training.append(current_model_train_distr)   \n",
    "    print('The restored model {} was trained using distr: {}\\n'.format(idx_model, current_model_train_distr))\n",
    "\n",
    "# test each model on each of the above distributions\n",
    "print('\\n\\n\\n--- test each model on all the above distributions ---\\n')\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    for idx_distr, distr in enumerate(distrs_used_for_training):\n",
    "        # reload every time the original dataset in order to ensure that we build the subset starting from the same point\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "#         mnist_ds.test.shuffle()\n",
    "#         mnist_ds.test.shuffle()\n",
    "#         mnist_ds.test.shuffle()\n",
    "        temp_model = Lenet5(mnist_ds, display_summary=False)\n",
    "        temp_model.restore_session(ckpt_dir=ckpt_dir, ckpt_filename=ckpt_file)\n",
    "        restored_train_distr = temp_model.session.run(temp_model.train_distr)\n",
    "        print('Restoring model from {}'.format(ckpt_file))\n",
    "        temp_model.restore_session(ckpt_dir=ckpt_dir, ckpt_filename=ckpt_file)\n",
    "        print('train_distr: {}'.format(restored_train_distr))\n",
    "        print('test_distr: {}'.format(distr))\n",
    "        mnist_ds.impose_distribution(np.array(distr), np.max(distrs_used_for_training))\n",
    "        test_loss, test_acc, total_predict, total_actual, wrong_predict_images, _= temp_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "        perf_dict['idx_model'].append(idx_model)\n",
    "        perf_dict['idx_distr'].append(idx_distr)\n",
    "        perf_dict['test_loss'].append(test_loss)\n",
    "        perf_dict['test_acc'].append(test_acc)\n",
    "        perf_dict['total_predict'].append(total_predict)\n",
    "        perf_dict['total_actual'].append(total_actual)\n",
    "        correct_predict = total_predict[total_actual == total_predict]\n",
    "        wrong_predict = total_predict[total_actual != total_predict]\n",
    "        wrong_actual = total_actual[total_actual != total_predict]\n",
    "        perf_dict['correct_predicted_distr'].append(np.histogram(correct_predict)[0])\n",
    "        perf_dict['wrong_predicted_distr'].append(np.histogram(wrong_predict)[0])\n",
    "        perf_dict['wrong_actual_distr'].append(np.histogram(wrong_actual)[0])\n",
    "        perf_dict['train_distr'].append(distr)\n",
    "        perf_dict['test_distr'].append(mnist_ds.test.label_distr)\n",
    "        perf_dict['ckpt_file'].append(ckpt_file)\n",
    "        print('idx_model = {}, idx_distr = {}: test_loss = {:.3f}, test_acc = {:.3f} ({}/{})\\n\\n\\n'.format(idx_model, idx_distr, test_loss, test_acc, mnist_ds.test.num_examples - len(wrong_predict_images), mnist_ds.test.num_examples))\n",
    "        \n",
    "# save the above results dictionary to file\n",
    "filename = 'results_{}.dict.pickle'.format(Utils.now_as_str())\n",
    "full_filepath = os.path.join(ckpt_dir, filename)\n",
    "filehandler = open(full_filepath, 'wb') \n",
    "pickle.dump(perf_dict, filehandler)\n",
    "print('Results dictionary was succesfully saved to: {}'.format(full_filepath))\n",
    "filehandler.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# restore and analyze the results from the above dictionary obtained by testing the models on all distributions\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5/'\n",
    "# filename = 'results_2018_03_18---20_56.dict.pickle'\n",
    "# filename = 'results_2018_03_20---12_33.dict.pickle' # after 1 test dataset shuffles \n",
    "# filename = 'results_2018_03_20---12_42.dict.pickle' # after 2 test dataset shuffles\n",
    "# filename = 'results_2018_03_20---12_55.dict.pickle' # after 3 test dataset shuffles\n",
    "\n",
    "\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5_2/'\n",
    "# filename = 'results_2018_03_20---14_28.dict.pickle'\n",
    "\n",
    "\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5_3/'\n",
    "# filename = 'results_2018_03_20---23_26.dict.pickle'\n",
    "\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5_augmented/'\n",
    "# filename = 'results_2018_03_21---11_54.dict.pickle'\n",
    "# filename = 'results_2018_05_08---00_42.dict.pickle'\n",
    "\n",
    "work_dir = './results/PriorProbabilityShift_experiment_5_10000samples/'\n",
    "# filename = 'results_2018_05_07---11_38.dict.pickle'\n",
    "filename = 'results_2018_05_08---01_03.dict.pickle'\n",
    "\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5_5000samples/'\n",
    "# filename = 'results_2018_05_07---12_17.dict.pickle'\n",
    "\n",
    "\n",
    "filehandler = open(os.path.join(work_dir,filename), 'rb') \n",
    "restored_perf_dict = pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "print('Results dictionary was succesfully restored from: {}'.format(filename))\n",
    "\n",
    "# build a pandas dataframe from results dictionary\n",
    "perf_df = pd.DataFrame(restored_perf_dict, columns=list(restored_perf_dict.keys()))\n",
    "display(perf_df.describe())\n",
    "display(perf_df)\n",
    "\n",
    "# build and plot the accuracy comparison matrix\n",
    "L = np.sqrt(len(perf_df)).astype(np.int32)\n",
    "acc_matrix = np.array(perf_df['test_acc']).reshape((L, L))\n",
    "distrs_used_for_training = perf_df['train_distr'][perf_df['idx_model'] == perf_df['idx_distr']]\n",
    "acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=acc_matrix)\n",
    "acc_matrix_plt.savefig(os.path.join(work_dir,filename + '.acc_matrix.png'))\n",
    "\n",
    "# build and plot distributions corresponding to wrong predictions\n",
    "wrong_predicted_distr_matrix = np.array(perf_df['wrong_predicted_distr']).reshape((L, L))\n",
    "distrs_used_for_training = perf_df['train_distr'][perf_df['idx_model'] == perf_df['idx_distr']]\n",
    "acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=acc_matrix, distr_matrix=wrong_predicted_distr_matrix)\n",
    "acc_matrix_plt.savefig(os.path.join(work_dir,filename + '.wrong_predictions_distr_matrix.png'))\n",
    "\n",
    "# build and plot distributions corresponding to correct predictions\n",
    "wrong_predicted_distr_matrix = np.array(perf_df['correct_predicted_distr']).reshape((L, L))\n",
    "distrs_used_for_training = perf_df['train_distr'][perf_df['idx_model'] == perf_df['idx_distr']]\n",
    "acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=acc_matrix, distr_matrix=wrong_predicted_distr_matrix)\n",
    "acc_matrix_plt.savefig(os.path.join(work_dir,filename + '.correct_predictions_distr_matrix.png'))\n",
    "\n",
    "# build and plot distributions corresponding to wrong actual predictions\n",
    "wrong_predicted_distr_matrix = np.array(perf_df['wrong_actual_distr']).reshape((L, L))\n",
    "distrs_used_for_training = perf_df['train_distr'][perf_df['idx_model'] == perf_df['idx_distr']]\n",
    "acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=acc_matrix, distr_matrix=wrong_predicted_distr_matrix)\n",
    "acc_matrix_plt.savefig(os.path.join(work_dir,filename + '.wrong_actual_predictions_distr_matrix.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze average acc matrix\n",
    "work_dir = './results/PriorProbabilityShift_experiment_5/'\n",
    "dict_file_list = get_all_files_from_dir_ending_with(work_dir, \"dict.pickle\")\n",
    "acc_matrices_list = [] # in order to make a comparison between results\n",
    "plot = False\n",
    "for dict_file in dict_file_list:\n",
    "    # restore the results dictionary\n",
    "    filename = os.path.join(work_dir, dict_file)\n",
    "    filehandler = open(filename, 'rb') \n",
    "    restored_perf_dict = pickle.load(filehandler)\n",
    "    filehandler.close()\n",
    "    print('Results dictionary was succesfully restored from: {}'.format(filename))\n",
    "    \n",
    "    # build a pandas dataframe from results dictionary\n",
    "    perf_df = pd.DataFrame(restored_perf_dict, columns=list(restored_perf_dict.keys()))\n",
    "    display(perf_df.describe())\n",
    "    display(perf_df)\n",
    "    \n",
    "    # build a matrix with all test accuracies and plot it\n",
    "    print(len(perf_df))\n",
    "    L = np.sqrt(len(perf_df)).astype(np.int32)\n",
    "    acc_matrix = np.array(perf_df['test_acc']).reshape((L, L))\n",
    "    acc_matrices_list.append(acc_matrix)\n",
    "    if plot:\n",
    "        acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=acc_matrix)\n",
    "        acc_matrix_plt.savefig('{}.acc_matrix.png'.format(filename))\n",
    "\n",
    "acc_matrices = np.array(acc_matrices_list)\n",
    "avg_acc_matrix = np.average(acc_matrices, axis = 0)\n",
    "acc_matrix_plt = Utils.plot_acc_matrix(train_distributions=distrs_used_for_training, acc_matrix=avg_acc_matrix)\n",
    "acc_matrix_plt.savefig(os.path.join(work_dir, 'average_acc_matrix.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the trained models on the entire MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare the first column of the above accuracy matrix with accuracies obtained by testing the models on the entire MNIST dataset\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5/'\n",
    "work_dir = './results/PriorProbabilityShift_experiment_5_10000samples/'\n",
    "ckpt_file_list = get_all_files_from_dir_ending_with(work_dir, \"ckpt.meta\", without_file_extension=True)\n",
    "perf_dict = {'idx_model':[], 'idx_distr':[], 'test_loss':[], 'test_acc':[], 'total_predict':[], 'total_actual':[], 'train_distr':[], 'test_distr':[], 'ckpt_file':[]}\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "    temp_model = Lenet5(mnist_ds, display_summary=False)\n",
    "    print('Restoring model from {}'.format(ckpt_file))\n",
    "    temp_model.restore_session(ckpt_dir=work_dir, ckpt_filename=ckpt_file)\n",
    "    current_model_train_distr = temp_model.session.run(temp_model.train_distr)\n",
    "    print('train_distr: {}'.format(current_model_train_distr))\n",
    "    test_loss, test_acc, total_predict, total_actual, wrong_predict_images, _= temp_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "    perf_dict['idx_model'].append(idx_model)\n",
    "    perf_dict['idx_distr'].append(-1)\n",
    "    perf_dict['test_loss'].append(test_loss)\n",
    "    perf_dict['test_acc'].append(test_acc)\n",
    "    perf_dict['total_predict'].append(total_predict)\n",
    "    perf_dict['total_actual'].append(total_actual)\n",
    "    perf_dict['train_distr'].append(current_model_train_distr)\n",
    "    perf_dict['test_distr'].append(mnist_ds.test.label_distr)\n",
    "    perf_dict['ckpt_file'].append(ckpt_file)\n",
    "    print('idx_model = {}, idx_distr = {}: test_loss = {:.4f}, test_acc = {:.4f} ({}/{})\\n\\n\\n'.format(idx_model, -1, test_loss, test_acc, mnist_ds.test.num_examples - len(wrong_predict_images), mnist_ds.test.num_examples))\n",
    "        \n",
    "# save the above results dictionary to file\n",
    "filename = 'results_from_testing_on_the_entire_testset.dict2.pickle'\n",
    "filehandler = open(os.path.join(work_dir,filename), 'wb') \n",
    "pickle.dump(perf_dict, filehandler)\n",
    "print('Results dictionary was succesfully saved to: {}'.format(filename))\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore and analyze the results dictionary obtained by testing the models on the entire testset\n",
    "# work_dir = './results/PriorProbabilityShift_experiment_5/'\n",
    "work_dir = './results/PriorProbabilityShift_experiment_5_10000samples/'\n",
    "filename = 'results_from_testing_on_the_entire_testset.dict2.pickle'\n",
    "filehandler = open(os.path.join(work_dir,filename), 'rb')\n",
    "restored_perf_dict = pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "print('Results dictionary was succesfully restored from: {}{}'.format(work_dir, filename))\n",
    "\n",
    "# build a pandas dataframe from results dictionary\n",
    "perf_df = pd.DataFrame(restored_perf_dict, columns=list(restored_perf_dict.keys()))\n",
    "display(perf_df.describe())\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test the models on the entire MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_and_test_a_model_on_a_mnist_subset(mnist_subset, ckpt_dir, ckpt_filemame, plot_filename):\n",
    "    print('Restoring model from {}{}'.format(ckpt_dir, ckpt_filemame))\n",
    "    restored_model = Lenet5(mnist_subset,display_summary=False)\n",
    "    restored_model.restore_session(ckpt_dir=ckpt_dir, ckpt_filename=ckpt_filemame)\n",
    "    train_distr = restored_model.session.run(restored_model.train_distr)\n",
    "    test_loss, test_acc, total_predict, total_actual, wrong_predict_images, total_softmax_output_probs = restored_model.test_data(mnist_subset.test)\n",
    "\n",
    "    print('test_loss = {:.4f}, test_acc = {:.4f} ({}/{})'.format(test_loss, test_acc, mnist_subset.test.num_examples - len(wrong_predict_images), mnist_subset.test.num_examples))\n",
    "    \n",
    "    # sort wrong_predict_images by target label\n",
    "    correct_predict = total_predict[total_actual == total_predict]\n",
    "    wrong_predict = total_predict[total_actual != total_predict]\n",
    "    wrong_predict_softmax_output_probs = total_softmax_output_probs[total_actual != total_predict]\n",
    "    wrong_actual = total_actual[total_actual != total_predict]\n",
    "    wrong_predict_images = np.array(wrong_predict_images)\n",
    "    wrong_predict_images_sorted = wrong_predict_images[wrong_actual.argsort(), ]\n",
    "    wrong_predict_images_sorted = [image for image in wrong_predict_images_sorted]\n",
    "\n",
    "    count_figures = 6\n",
    "    fig = plt.figure(figsize=(30, 3))\n",
    "    fig.suptitle(y = 1.1, t = 'test_acc = {:.4f} ({}/{})'.format(test_acc, mnist_subset.test.num_examples - len(wrong_predict_images), mnist_subset.test.num_examples), fontsize=18, fontweight='bold')\n",
    "\n",
    "    k = 1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(10), train_distr)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('train label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(10), mnist_subset.test.label_distr)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('test label distr')\n",
    "\n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(correct_predict, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('correct predicted label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(wrong_predict, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('wrong predicted label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.hist(wrong_actual, bins=np.arange(11), rwidth=0.8, normed=False)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('wrong actual label distr')\n",
    "    \n",
    "    k+=1\n",
    "    plt.subplot(1,count_figures, k)\n",
    "    plt.bar(range(0, 10), np.average(wrong_predict_softmax_output_probs, axis=0))\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('average of wrong actual softmax output probabilities')\n",
    "\n",
    "    plt.savefig(os.path.join(ckpt_dir, plot_filename))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ckpt_dir = './results/PriorProbabilityShift_experiment_5/'\n",
    "ckpt_dir = './results/PriorProbabilityShift_experiment_5_10000samples/'\n",
    "ckpt_file_list = get_all_files_from_dir_ending_with(ckpt_dir, \"ckpt.meta\", without_file_extension=True)\n",
    "for idx, ckpt_file in enumerate(ckpt_file_list):\n",
    "    mnist_ds = MNISTDataset(MNIST_TRAIN_IMAGES_FILEPATH, MNIST_TRAIN_LABELS_FILEPATH, MNIST_TEST_IMAGES_FILEPATH, MNIST_TEST_LABELS_FILEPATH)\n",
    "    # test on all original data distribution, without imposing any distribution\n",
    "    restore_and_test_a_model_on_a_mnist_subset(mnist_ds, ckpt_dir=ckpt_dir, ckpt_filemame=ckpt_file, plot_filename = 'tested_on_all_data_{}'.format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs. In many situations, wrong predicted distributions are correlated to train distributions (especially when train distribution is very skewed). So the model tends to predict based what it has seen the most during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
