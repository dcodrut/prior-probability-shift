{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Local Adaptive Model - Notebook 3_2\n",
    "\n",
    "* Prior Probability Shift is one of the common problems encountered in Machine Learning algortihms.   \n",
    "* There are some approaches for dealing with this problem in a 'static' scenario. But there are situations in which we need a model which deals with secvential data as input (e.g. a server which gets input from different users, with different data distributions).   \n",
    "* In this project, we try to build a model which self adapts its predictions based on the local label distribution. \n",
    "\n",
    "### About notebook 3_2\n",
    "\n",
    "In this notebook we address the problem of Prior Probability Shift. In Experiment 2, a modified LeNet5 model is trained in order to make it able to adapt to local distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup and data preparation\n",
    "Launch notebook in Windows from terminal:  C:\\Users\\diaco\\Anaconda3\\envs\\work\\python.exe -m notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "% matplotlib inline\n",
    "# %matplotlib qt\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_context('notebook')\n",
    "# sns.set_style('white')  # workaround for displaying plot axis on white background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# os.chdir(r'C:\\Users\\diaco\\Desktop\\ML\\Licenta\\finalproject\\mnist')\n",
    "from dataset import MNISTDataset\n",
    "from utils import Utils\n",
    "from lenet5 import Lenet5\n",
    "from lenet5_with_distr import Lenet5WithDistr\n",
    "import PIL.Image\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(edgeitems=10)\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random generator using a constant seed in order to reproduce results\n",
    "seed = 112358\n",
    "nprg = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_TRAIN_IMG_PATH = 'MNIST_dataset/train-images.idx3-ubyte'\n",
    "MNIST_TRAIN_LABELS_PATH = 'MNIST_dataset/train-labels.idx1-ubyte'\n",
    "MNIST_TEST_IMG_PATH = 'MNIST_dataset/t10k-images.idx3-ubyte'\n",
    "MNIST_TEST_LABELS_PATH = 'MNIST_dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "mnist_ds = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - build a list with distributions used in previously trained models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_10000samples/\"\n",
    "WORK_DIR = \"./results/PriorProbabilityShift_experiment_5_5000samples/\"\n",
    "ckpt_file_list = Utils.get_all_files_from_dir_ending_with(WORK_DIR, \"ckpt.meta\", without_file_extension=True)\n",
    "perf_dict = {'idx_model': [], 'idx_distr': [], 'test_loss': [], 'test_acc': [], 'total_predict': [], 'total_actual': [],\n",
    "             'train_distr': [], 'test_distr': [], 'ckpt_file': []}\n",
    "\n",
    "# build a list with all ditributions considered in training phase\n",
    "distrs_used_for_training = []\n",
    "for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "    print('Restoring model {} from {}'.format(idx_model, ckpt_file))\n",
    "    temp_model = Lenet5(mnist_dataset=mnist_ds, display_summary=False)\n",
    "    temp_model.restore_session(ckpt_dir=WORK_DIR, ckpt_filename=ckpt_file)\n",
    "    current_model_train_distr = temp_model.session.run(temp_model.train_distr)\n",
    "    distrs_used_for_training.append(current_model_train_distr)\n",
    "    print('The restored model {} was trained using distr: {}\\n'.format(idx_model, current_model_train_distr))\n",
    "    plt.bar(range(0, 10), current_model_train_distr)\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('current_model_train_distr')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2. Try to adapt the standard version of LeNet5 in order to make the model able to adapt to local label distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1\n",
    "Train 2 models:\n",
    "- first one without using the distribution as input\n",
    "- second one using the distribution as input but without explicitly build batches with respect to a distribution (i.e. just append the distribution of current batch before the specified layers)\n",
    "\n",
    "Check if appending the distribution has an influence on the accuracy.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE_LIST = [150, 250, 500, 1000, 5000, 10000, 45000]\n",
    "\n",
    "DISTR_POS_LIST = [\n",
    "    [False, False, False, False, False],\n",
    "    [False, False, False, False, True],\n",
    "    [False, False, True, False, False],\n",
    "    [False, False, True, True, True],\n",
    "    [False, True, False, False, False],\n",
    "    [True, False, False, False, False],\n",
    "    [True, True, False, False, False],\n",
    "    [True, True, True, True, True]\n",
    "]\n",
    "for subset_size in SUBSET_SIZE_LIST:\n",
    "    if subset_size < 500:\n",
    "        batch_size = 50\n",
    "    else:\n",
    "        batch_size = 100\n",
    "        \n",
    "    for distr_pos in DISTR_POS_LIST:\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n",
    "        mnist_ds.impose_distr_on_train_dataset(subset_size=subset_size, weights=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "        lenet5_model_with_distr = Lenet5WithDistr(mnist_ds, \"randDistr_distrPos_{}_{}examples_{}batchSize\".format(distr_pos, subset_size, batch_size),\n",
    "                                                  epochs=40, batch_size=batch_size, variable_mean=0, variable_stddev=0.1,\n",
    "                                                  learning_rate=0.001,\n",
    "                                                  drop_out_keep_prob=0.75,\n",
    "                                                  distr_pos=distr_pos)\n",
    "        lenet5_model_with_distr.train(distrs_list=None)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Section 2\n",
    "Train a model by imposing only those eight distributions used in the first experiment. Use the same amount of data.  \n",
    "Check its accuracy comparing to the models from the first emperiment (adjusting with real priors) and to the models trained with distributions appended as they are.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE_LIST = [150, 250, 500, 1000, 5000, 10000, 45000]\n",
    "\n",
    "DISTR_POS_LIST = [\n",
    "    [False, False, False, False, False],\n",
    "    [False, False, False, False, True],\n",
    "    [False, False, True, False, False],\n",
    "    [False, False, True, True, True],\n",
    "    [False, True, False, False, False],\n",
    "    [True, False, False, False, False],\n",
    "    [True, True, False, False, False],\n",
    "    [True, True, True, True, True]\n",
    "]\n",
    "\n",
    "for subset_size in SUBSET_SIZE_LIST: \n",
    "    if subset_size < 500:\n",
    "        batch_size = 50\n",
    "    else:\n",
    "        batch_size = 100\n",
    "        \n",
    "    for distr_pos in DISTR_POS_LIST:\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n",
    "        mnist_ds.impose_distr_on_train_dataset(subset_size=subset_size, weights=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "        lenet5_model_with_distr = Lenet5WithDistr(mnist_ds, \"8distrs_distrPos_{}_{}examples_{}batchSize\".format(distr_pos, subset_size, batch_size),\n",
    "                                                  epochs=40, batch_size=batch_size, variable_mean=0, variable_stddev=0.1,\n",
    "                                                  learning_rate=0.001,\n",
    "                                                  drop_out_keep_prob=0.75,\n",
    "                                                  distr_pos=distr_pos)\n",
    "        lenet5_model_with_distr.train(distrs_list=distrs_used_for_training)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - restore and test the adapted models on subsets with different distributions; save the results to file\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WORK_DIRS = [\n",
    "#     \"./results/Lenet5WithDistr_8distr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_10000samples_2/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_allData/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_allData/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_10000samples_2/\"]\n",
    "\n",
    "# WORK_DIRS = [\"./results/Lenet5WithDistr_8distr_10000samples_2/\"]\n",
    "\n",
    "# WORK_DIRS = [\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_150samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_250samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_500samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_1000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_5000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_45000samples/\"\n",
    "# ]\n",
    "\n",
    "# WORK_DIRS = [\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_150samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_250samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_500samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_1000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_5000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_45000samples/\"\n",
    "# ]\n",
    "\n",
    "WORK_DIRS = [\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/150samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/250samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/500samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/1000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/5000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/10000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/45000samples/\"\n",
    "]\n",
    "\n",
    "NO_TESTS_PER_MODEL = 50\n",
    "TEST_SUBSET_SIZE = 3000\n",
    "SAVE_DICT_TO_FILE = True\n",
    "USE_SHORTCUT_METHOD_FOR_TESTING = True\n",
    "USE_NORMAL_METHOD_FOR_TESTING = False\n",
    "\n",
    "def save_test_results_to_dict(test_dict, test_method, test_size, id_test, id_distr, test_distr, test_acc, total_actual, total_predict):\n",
    "    test_dict['test_method'].append(test_method)\n",
    "    test_dict['testset_size'].append(test_size)\n",
    "    test_dict['id_test'].append(id_test)\n",
    "    test_dict['id_distr'].append(id_distr)\n",
    "    test_dict['test_distr'].append(test_distr)\n",
    "    test_dict['test_acc'].append(test_acc)\n",
    "    correct_predict = total_predict[total_actual == total_predict]\n",
    "    wrong_predict = total_predict[total_actual != total_predict]\n",
    "    wrong_actual = total_actual[total_actual != total_predict]\n",
    "    test_dict['correct_predicted_counts'].append(np.bincount(correct_predict, minlength=10))\n",
    "    test_dict['wrong_predicted_counts'].append(np.bincount(wrong_predict, minlength=10))\n",
    "    test_dict['wrong_actual_counts'].append(np.bincount(wrong_actual, minlength=10))\n",
    "        \n",
    "for WORK_DIR in WORK_DIRS:\n",
    "\n",
    "    ckpt_file_list = Utils.get_all_files_from_dir_ending_with(WORK_DIR, \"ckpt.meta\", without_file_extension=True)\n",
    "\n",
    "    # dictionary used for saving all the results\n",
    "    full_results_dict = {}\n",
    "\n",
    "    for idx_model, ckpt_file in enumerate(ckpt_file_list):\n",
    "        print('{}: Restoring model {} from {}{}'.format(Utils.now_as_str(), idx_model, WORK_DIR, ckpt_file))\n",
    "\n",
    "        restored_distr_pos = Utils.restore_variable_from_checkpoint(ckpt_dir=WORK_DIR, ckpt_file=ckpt_file, var_name='distr_pos')\n",
    "        test_model = Lenet5WithDistr(mnist_dataset=mnist_ds, verbose=False, distr_pos=restored_distr_pos)\n",
    "        test_model.restore_session(ckpt_dir=WORK_DIR, ckpt_filename=ckpt_file)\n",
    "        current_model_train_distr = test_model.session.run(test_model.train_distr)\n",
    "        full_results_dict[ckpt_file] = {'idx_model':idx_model, 'distr_pos':restored_distr_pos, 'train_distr': current_model_train_distr, \n",
    "                                   'test_results': {'test_method':[], 'testset_size':[], 'id_test':[], 'id_distr':[], 'test_distr':[], 'test_acc':[],\n",
    "                                                    'correct_predicted_counts':[],    'wrong_predicted_counts':[], 'wrong_actual_counts':[]}}\n",
    "\n",
    "        print('The restored model {} was trained using distr: {}, but with batch distr. attached (distr_pos = {})\\n'.format(idx_model, current_model_train_distr, restored_distr_pos))\n",
    "\n",
    "        # test on the entire MNIST dataset\n",
    "        mnist_ds = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n",
    "        print('Testing model on the entire dataset, with its distr. attached ({})'.format(mnist_ds.test.label_distr))\n",
    "        test_loss, test_acc, total_predict, total_actual, wrong_predict_images, output_probs = test_model.test_data(mnist_ds.test, use_only_one_batch=True)\n",
    "        print('test_acc = {:.3f}% ({}/{})\\n'.format(test_acc * 100, mnist_ds.test.num_examples - len(wrong_predict_images), mnist_ds.test.num_examples))\n",
    "\n",
    "        # save the test results\n",
    "        save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='entire', test_size=mnist_ds.test.num_examples, id_test=-1, id_distr=-1, \n",
    "                                  test_distr=mnist_ds.test.label_distr, test_acc=test_acc, total_actual=total_actual, total_predict=total_predict)\n",
    "        \n",
    "        print('{}: start testing model on distributions\\n'.format(Utils.now_as_str()))\n",
    "        for idx_distr, distr in enumerate(distrs_used_for_training):\n",
    "            print('Testing model on the entire dataset, with {} attached'.format(distr))\n",
    "            mnist_ds = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n",
    "            test_loss, test_acc, total_predict, total_actual, wrong_predict_images, output_probs = test_model.test_data( mnist_ds.test, use_only_one_batch=True, distr_to_attach=distr)\n",
    "            print('test_acc = {:.3f}% ({}/{})'.format(test_acc * 100, mnist_ds.test.num_examples - len(wrong_predict_images), mnist_ds.test.num_examples))\n",
    "\n",
    "            # save the test results\n",
    "            save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='entire_and_distr_attached', test_size=mnist_ds.test.num_examples, id_test=-1, id_distr=idx_distr, \n",
    "                                      test_distr=distr, test_acc=test_acc, total_actual=total_actual, total_predict=total_predict)\n",
    "\n",
    "            print('Testing model {} ({} times) on distribution {}: {}'.format(idx_model, NO_TESTS_PER_MODEL, idx_distr, distr))\n",
    "            \n",
    "            # compute theoretical mean accuracy\n",
    "            wrong_actual = total_actual[total_actual != total_predict]\n",
    "            theoretical_wrong_count = np.sum(np.bincount(wrong_actual, minlength=10) * (10 * distr)) * (TEST_SUBSET_SIZE / mnist_ds.test.num_examples)\n",
    "            print('theoretical \\u03BC(acc) = {:.3f}% ({}/{})'.format((1 - theoretical_wrong_count / TEST_SUBSET_SIZE) * 100, TEST_SUBSET_SIZE - theoretical_wrong_count, TEST_SUBSET_SIZE))\n",
    "            \n",
    "            if USE_SHORTCUT_METHOD_FOR_TESTING:\n",
    "                list_acc_shortcut_method = []\n",
    "                for id_test in range(NO_TESTS_PER_MODEL):\n",
    "                    indices_wrt_distr = Utils.get_indices_wrt_distr(labels=total_actual, weights=distr, max_no_examples=TEST_SUBSET_SIZE)\n",
    "                    num_examples_selection = len(indices_wrt_distr)\n",
    "                    total_predict_selection = total_predict[indices_wrt_distr]\n",
    "                    total_actual_selection = total_actual[indices_wrt_distr]\n",
    "                    no_correct_predicted = np.sum(total_predict_selection == total_actual_selection)\n",
    "                    test_acc = no_correct_predicted / num_examples_selection\n",
    "#                     print('id_test = {} : test_acc = {:.3f}% ({}/{})'.format(id_test, test_acc * 100, no_correct_predicted, num_examples_selection))\n",
    "\n",
    "                    list_acc_shortcut_method.append(test_acc)\n",
    "\n",
    "                    # save the test results\n",
    "                    save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='shortcut', test_size=num_examples_selection, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=test_acc, total_actual=total_actual_selection, total_predict=total_predict_selection)\n",
    "            \n",
    "                print('\\u03BC(acc_s ) = {:.3f}%, \\u03C3(acc_s ) = {:.3f}%'.format(np.mean(list_acc_shortcut_method) * 100, np.std(list_acc_shortcut_method, ddof=1) * 100))\n",
    "            \n",
    "            if USE_NORMAL_METHOD_FOR_TESTING:\n",
    "                list_acc_normal_method = []\n",
    "                for id_test in range(NO_TESTS_PER_MODEL):\n",
    "                    mnist_ds1 = MNISTDataset(MNIST_TRAIN_IMG_PATH, MNIST_TRAIN_LABELS_PATH, MNIST_TEST_IMG_PATH, MNIST_TEST_LABELS_PATH)\n",
    "                    for s in range(id_test):\n",
    "                        mnist_ds1.test.shuffle() # when MNISTDataset is instanced, the rg is reseted, so explicitly shuffling is needed for getting different results\n",
    "                    mnist_ds1.impose_distribution(weights=distr, max_test_size=TEST_SUBSET_SIZE)\n",
    "                    test_loss1, test_acc1, total_predict1, total_actual1, wrong_predict_images1, output_probs1 = test_model.test_data(mnist_ds1.test, use_only_one_batch=True)\n",
    "#                     print('id_test = {} : test_acc1 = {:.3f}% ({}/{})'.format(id_test, test_acc1 * 100, mnist_ds1.test.num_examples - len(wrong_predict_images1), mnist_ds1.test.num_examples))\n",
    "\n",
    "                    list_acc_normal_method.append(test_acc1)\n",
    "\n",
    "                    # save the test results\n",
    "                    save_test_results_to_dict(test_dict=full_results_dict[ckpt_file]['test_results'], test_method='normal', test_size=mnist_ds1.test.num_examples, id_test=id_test, id_distr=idx_distr, \n",
    "                                              test_distr=distr, test_acc=test_acc1, total_actual=total_actual1, total_predict=total_predict1)\n",
    "\n",
    "                print('\\u03BC(acc_n) = {:.3f}%, \\u03C3(acc_n) = {:.3f}%'.format(np.mean(list_acc_normal_method) * 100, np.std(list_acc_normal_method, ddof=1) * 100))\n",
    "                \n",
    "            print('\\n')\n",
    "\n",
    "        # analyze error distribution\n",
    "        # restore_and_test_a_model_on_a_mnist_subset(mnist_ds, ckpt_dir=WORK_DIR, ckpt_filemame=ckpt_file, plot_filename = 'test_model')\n",
    "\n",
    "    # save the above results dictionary to file\n",
    "    if SAVE_DICT_TO_FILE:\n",
    "        filename = 'testing_results_{}testsPerModel.dict.pickle'.format(NO_TESTS_PER_MODEL)\n",
    "        full_filepath = os.path.join(WORK_DIR, filename)\n",
    "        filehandler = open(full_filepath, 'wb') \n",
    "        pickle.dump(full_results_dict, filehandler)\n",
    "        print('Results dictionary was succesfully saved to: {}'.format(full_filepath))\n",
    "        filehandler.close()\n",
    "        \n",
    "    print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - check if testing on the entire MNIST testset with different distributions attached affects the accuracy\n",
    "- analyze the wrong predictions distribution\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK_DIR = \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_1000samples/\"\n",
    "# filename = 'testing_results_50testsPerModel.dict.pickle'\n",
    "# ckpt_file = 'Lenet5_8distrs_distrPos_[False, False, True, True, True]_1000examples_2018_05_28---11_02.model.ckpt'\n",
    "\n",
    "WORK_DIR = \"./results/Lenet5WithDistr_8distr_[]samples_2/Lenet5_8distr_1000samples/\"\n",
    "filename = 'testing_results_50testsPerModel.dict.pickle'\n",
    "ckpt_file = 'Lenet5_8distrs_distrPos_[False, False, True, True, True]_1000examples_100batchSize_2018_06_01---11_05.model.ckpt'\n",
    "\n",
    "filehandler = open(os.path.join(WORK_DIR, filename), 'rb')\n",
    "restored_full_results_dict = pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "print('Results dictionary was succesfully restored from: {}{}'.format(WORK_DIR, filename))\n",
    "\n",
    "print('idx_model = {}, ckpt_file = {}\\n'.format(restored_full_results_dict[ckpt_file]['idx_model'], ckpt_file))\n",
    "perf_df = pd.DataFrame(restored_full_results_dict[ckpt_file]['test_results'], columns=list(restored_full_results_dict[ckpt_file]['test_results'].keys()))\n",
    "temp_df = perf_df[(perf_df['test_method'] == 'entire_and_distr_attached')]\n",
    "# assert(False)\n",
    "\n",
    "k = 0\n",
    "fig = plt.figure(figsize=(40, 7))\n",
    "fig.suptitle(t='ckpt_file = {}'.format(ckpt_file), fontsize=20, fontweight='bold')\n",
    "no_subplots_per_row = temp_df.shape[0]\n",
    "\n",
    "global_max_wrong_predicted_count = 0\n",
    "for index, df_row in temp_df.iterrows():\n",
    "    if np.max(df_row['wrong_predicted_counts']) > global_max_wrong_predicted_count:\n",
    "        global_max_wrong_predicted_count = np.max(df_row['wrong_predicted_counts'])\n",
    "\n",
    "for index, df_row in temp_df.iterrows():\n",
    "#     print(df_row['test_distr'])\n",
    "    k += 1\n",
    "    \n",
    "    plt.subplot(2, no_subplots_per_row, k)\n",
    "    plt.bar(range(10), df_row['test_distr'] / sum(df_row['test_distr']))\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.title('acc = {:.1f}%'.format(df_row['test_acc'] * 100), fontsize=16)\n",
    "    if k==1:\n",
    "        plt.ylabel('attached distr.', fontsize=20)\n",
    "    \n",
    "    plt.subplot(2, no_subplots_per_row, k + no_subplots_per_row)\n",
    "    plt.ylim([0, global_max_wrong_predicted_count])\n",
    "    plt.bar(range(10), df_row['wrong_predicted_counts'])\n",
    "    plt.xticks(range(0, 10))\n",
    "    plt.grid()\n",
    "    if k==1:\n",
    "        plt.ylabel('wrong pred. counts', fontsize=20)\n",
    "        \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- print results in a compact form\n",
    "- compare the different distr_pos's considered\n",
    "- check again if attaching the distribution has an impact on performance\n",
    "- compare the results between 8distr and randomDistr\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print(np.sort(-scores))\n",
    "print(np.argsort(-scores))\n",
    "print(np.searchsorted(np.sort(-scores), -scores[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORK_DIRS = [\n",
    "#     \"./results/Lenet5WithDistr_8distr_allData\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_allData\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_10000samples_2/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_10000samples_2/\"]\n",
    "\n",
    "\n",
    "# WORK_DIRS = [\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_150samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_250samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_500samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_1000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_5000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_8distr_[]samples/Lenet5_8distr_45000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_150samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_250samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_500samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_1000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_5000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_10000samples/\",\n",
    "#     \"./results/Lenet5WithDistr_randomDistr_[]samples/Lenet5_randomDistr_45000samples/\"\n",
    "# ]\n",
    "\n",
    "WORK_DIRS = [\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/150samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/250samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/500samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/1000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/5000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/10000samples/\",\n",
    "    \"./results/Lenet5WithDistr_8distr_[]samples_2/45000samples/\"\n",
    "]\n",
    "\n",
    "filename = 'testing_results_50testsPerModel.dict.pickle'\n",
    "\n",
    "\n",
    "def std_1ddof(x):\n",
    "    return np.std(x, ddof=1)\n",
    "\n",
    "for WORK_DIR in WORK_DIRS:\n",
    "    filehandler = open(os.path.join(WORK_DIR, filename), 'rb')\n",
    "    restored_full_results_dict = pickle.load(filehandler)\n",
    "    filehandler.close()\n",
    "    print('Results dictionary was succesfully restored from: {}{}'.format(WORK_DIR, filename))\n",
    "\n",
    "    # compute a score for each distr_pos in order to decide where to attach the distribution\n",
    "    acc_matrix_to_compare_distr_pos = []\n",
    "    list_distr_pos = []\n",
    "\n",
    "    # iterate trough models\n",
    "    for ckpt_file in restored_full_results_dict.keys():\n",
    "        perf_df = pd.DataFrame(restored_full_results_dict[ckpt_file]['test_results'], columns=list(restored_full_results_dict[ckpt_file]['test_results'].keys()))\n",
    "        temp_df = perf_df[(perf_df['test_method'] != 'entire_and_distr_attached')].groupby(['test_method', 'id_distr']).agg([np.mean, std_1ddof])\n",
    "        temp_acc_list = []\n",
    "        for id_distr in perf_df[perf_df['test_method'] != 'entire_and_distr_attached']['id_distr'].unique():    \n",
    "            temp_df = perf_df[(perf_df['test_method'] != 'entire_and_distr_attached') & (perf_df['id_distr'] == id_distr)]\n",
    "            temp_acc_list.append(np.average(temp_df['test_acc']))\n",
    "        acc_matrix_to_compare_distr_pos.append(temp_acc_list)\n",
    "        list_distr_pos.append(restored_full_results_dict[ckpt_file]['distr_pos'])\n",
    "        \n",
    "    acc_matrix_to_compare_distr_pos = np.array(acc_matrix_to_compare_distr_pos)\n",
    "    min_per_column = np.min(acc_matrix_to_compare_distr_pos, axis = 0)\n",
    "    acc_diff_matrix = acc_matrix_to_compare_distr_pos - min_per_column\n",
    "    acc_diff_matrix = acc_diff_matrix / np.max(acc_diff_matrix, axis = 0)\n",
    "    scores = np.sum(acc_diff_matrix, axis=1) / acc_diff_matrix.shape[1]\n",
    "    \n",
    "    # iterate trough models\n",
    "    for idx, ckpt_file in enumerate(restored_full_results_dict.keys()):\n",
    "    #     print('idx_model = {}, ckpt_file = {}'.format(restored_full_results_dict[ckpt_file]['idx_model'], ckpt_file))\n",
    "    #     print('distr_pos = {}'.format(restored_full_results_dict[ckpt_file]['distr_pos']))\n",
    "    #     print('train_distr = {}'.format(restored_full_results_dict[ckpt_file]['train_distr']))\n",
    "    #     print()\n",
    "\n",
    "        # build a pandas dataframe from results dictionary\n",
    "        perf_df = pd.DataFrame(restored_full_results_dict[ckpt_file]['test_results'], columns=list(restored_full_results_dict[ckpt_file]['test_results'].keys()))\n",
    "    #     display(perf_df)\n",
    "    #     display(perf_df.describe())\n",
    "    #     display(perf_df.head())\n",
    "\n",
    "    #     display(perf_df[(perf_df['test_method'] == 'normal') & (perf_df['id_distr'] == 0)])\n",
    "    #     display(perf_df[(perf_df['test_method'] == 'normal') & (perf_df['id_distr'] == 0)].describe())\n",
    "        temp_df = perf_df[(perf_df['test_method'] != 'entire_and_distr_attached')].groupby(['test_method', 'id_distr']).agg([np.mean, std_1ddof])\n",
    "    #     display(temp_df['test_acc'])\n",
    "\n",
    "        # plot results\n",
    "        print('{} --- '.format(restored_full_results_dict[ckpt_file]['distr_pos']), end=\"\", flush=True)\n",
    "        for id_distr in perf_df[perf_df['test_method'] != 'entire_and_distr_attached']['id_distr'].unique():    \n",
    "            temp_df = perf_df[(perf_df['test_method'] != 'entire_and_distr_attached') & (perf_df['id_distr'] == id_distr)]\n",
    "    #         display(temp_df.head())\n",
    "            print('{:.1f}% \\u00B1 {:.1f}%, '.format(np.average(temp_df['test_acc']) * 100, std_1ddof(temp_df['test_acc']) * 100), end=\"\", flush=True)\n",
    "        print(' --- score = {:.3f} --- {}'.format(scores[idx], np.searchsorted(np.sort(-scores), -scores[idx])+1))\n",
    "\n",
    "        \n",
    "    print('\\n\\n')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
